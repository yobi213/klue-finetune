{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respected-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "needed-egypt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe75463b228>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unsigned-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "senior-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset klue (/home/yobi/.cache/huggingface/datasets/klue/sts/1.0.0/55ff8f92b7a4b9842be6514ce0b4b5295b46d5e493f8bb5760da4be717018f90)\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"klue\", \"sts\")\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "# KLUE STS 내 훈련, 검증 데이터 예제 변환\n",
    "for phase in [\"train\", \"validation\"]:\n",
    "    examples = datasets[phase]\n",
    "\n",
    "    for example in examples:\n",
    "        score = float(example[\"labels\"][\"label\"]) / 5.0  # 0.0 ~ 1.0 스케일로 유사도 정규화\n",
    "\n",
    "        inp_example = InputExample(\n",
    "            texts=[example[\"sentence1\"], example[\"sentence2\"]], \n",
    "            label=score,\n",
    "        )\n",
    "\n",
    "        if phase == \"validation\":\n",
    "            dev_samples.append(inp_example)\n",
    "        else:\n",
    "            train_samples.append(inp_example)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_samples,\n",
    "    shuffle=True,\n",
    "    batch_size=train_batch_size,\n",
    ")\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    dev_samples,\n",
    "    name=\"sts-dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-healthcare",
   "metadata": {},
   "source": [
    "### train 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "plastic-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f90b86a2fa430dadfa615772f1ce90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2499736c20594b0c93664064d2d2bec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2dd1f599d748b4a6786bde406088f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d996e9efe84780a2298786e6dff19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f2744d02da46979e45af849b4f9e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = models.Transformer(\"klue/roberta-base\")\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")\n",
    "model = SentenceTransformer(modules=[embedding_model,  pooling_model])\n",
    "# dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "# model = SentenceTransformer(modules=[embedding_model,  pooling_model, dense_model])\n",
    "\n",
    "num_epochs = 4\n",
    "model_save_path = \"output/768\"\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1)  # 10% of train data for warm-up\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-crowd",
   "metadata": {},
   "source": [
    "### train 256 epochs-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "composed-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9dd6e291dc41fd86c0a95bb156b3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=8.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e96933cae1b4262999216e9966459a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3cfbfa8caf4d3ea0eb088e5c1e7a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758c5f698e594cd1be54f915d04bd6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa897c67f7f475fa8aa09f15092cfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0697f359bc52443c9576f83caa3ad124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2d2a8499f64317a4fb3eadea2e7382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ced117c0f84439926508ba2c5b26a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a513a8e1e4462b3ca9279f5bbfa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = models.Transformer(\"klue/roberta-base\")\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")\n",
    "# model = SentenceTransformer(modules=[embedding_model,  pooling_model])\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[embedding_model, pooling_model, dense_model])\n",
    "\n",
    "num_epochs = 8\n",
    "model_save_path = \"output/256\"\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1)  # 10% of train data for warm-up\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afraid-performance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b670f7d4640446b88f14c48dc31fc79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b38d88c91a401fb89c2ba2b49b8eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22945bef1925495d9536f5ad9e4a51c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4c04f80a4246de96b50a1bfc815688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0426f522fba34f019010dfae8ba377c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## load 768, add pool, add 256 epochs-4\n",
    "embedding_model = models.Transformer(\"./output/768/0_Transformer\")\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")\n",
    "# model = SentenceTransformer(modules=[embedding_model,  pooling_model])\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[embedding_model,  pooling_model, dense_model])\n",
    "\n",
    "num_epochs = 4\n",
    "model_save_path = \"output/768-pool-256\"\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1)  # 10% of train data for warm-up\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-publisher",
   "metadata": {},
   "source": [
    "### load 768-pool, add 256 epochs-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spatial-tackle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275140dfc7404ef2b27840d8f9ce47ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97230d5b8cc4f8e858521bf7d5c39cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c142893e3ff64fd09662eca3101b0e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b886854c4754ac6a66f5a70e9eaf9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078e84ee1f86403890ecf3f2a6b612e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=365.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('./output/768/')\n",
    "\n",
    "# pooling_model = models.Pooling(\n",
    "#     embedding_model.get_word_embedding_dimension(),\n",
    "#     pooling_mode_mean_tokens=True,\n",
    "#     pooling_mode_cls_token=False,\n",
    "#     pooling_mode_max_tokens=False,\n",
    "# )\n",
    "# model = SentenceTransformer(modules=[embedding_model,  pooling_model])\n",
    "dense_model = models.Dense(in_features=embedding_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[embedding_model, dense_model])\n",
    "\n",
    "num_epochs = 4\n",
    "model_save_path = \"output/768-256\"\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1)  # 10% of train data for warm-up\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-powell",
   "metadata": {},
   "source": [
    "### load custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "trying-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_256 = SentenceTransformer('./output/768-pool-256/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "verbal-picture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_256.encode('안녕'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "likely-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception when trying to download https://sbert.net/models/sentence-transformers/LaBSE.zip. Response 404\n",
      "SentenceTransformer-Model https://sbert.net/models/sentence-transformers/LaBSE.zip not found. Try to create it from scratch\n",
      "Try to create Transformer Model sentence-transformers/LaBSE with mean pooling\n"
     ]
    }
   ],
   "source": [
    "model_labse = SentenceTransformer('sentence-transformers/LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protective-harmony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90.2kB [00:00, 865kB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yobi/.cache/torch/sentence_transformers/drive.google.com_file_d_13iNZAp1CR125WxOkO11bPAmk9Y8izs_q_view?usp=sharing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/electra/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m                         \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                         \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                             \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/electra/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/electra/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b4fe86d4ce7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/file/d/13iNZAp1CR125WxOkO11bPAmk9Y8izs_q/view?usp=sharing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/electra/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device)\u001b[0m\n\u001b[1;32m     92\u001b[0m                             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/electra/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/electra/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yobi/.cache/torch/sentence_transformers/drive.google.com_file_d_13iNZAp1CR125WxOkO11bPAmk9Y8izs_q_view?usp=sharing'"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('https://drive.google.com/file/d/13iNZAp1CR125WxOkO11bPAmk9Y8izs_q/view?usp=sharing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "steady-server",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.encode('안녕'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "minute-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = models.Transformer(\"./output/768/0_Transformer\")\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")\n",
    "# model = SentenceTransformer(modules=[embedding_model,  pooling_model])\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model = SentenceTransformer(modules=[embedding_model,  pooling_model, dense_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smooth-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "complex-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_notpool = SentenceTransformer(modules=[embedding_model,  pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "amateur-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pool = SentenceTransformer(\"./output/768/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "western-tsunami",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_notpool.encode(\"hi\", convert_to_tensor=True) == model_pool.encode(\"hi\", convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "flush-dressing",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9588e-01, -2.6076e-02, -7.0387e-01, -1.9161e-01,  3.7078e-01,\n",
       "        -8.0734e-02, -1.2706e+00,  9.3710e-02,  5.2949e-02, -2.9981e-01,\n",
       "        -6.8515e-01, -8.1802e-01, -3.0679e-01,  1.6045e-01,  2.2344e-01,\n",
       "         2.4976e-01,  5.0006e-02,  2.0250e-01, -6.1771e-01, -1.4783e+00,\n",
       "         1.1222e-01, -2.0073e-01,  5.4832e-01,  4.0254e-02,  3.9147e-01,\n",
       "        -1.5457e-01,  1.7514e-01, -3.7323e-01,  1.4839e-01,  9.8344e-01,\n",
       "         8.8302e-02,  4.1626e-01, -5.2299e-01, -3.5881e-01,  7.5960e-01,\n",
       "        -2.4401e-01, -2.6519e-01, -6.7280e-01, -2.1843e-01, -1.8963e-01,\n",
       "        -4.8162e-01, -6.0493e-02, -2.8714e-01, -8.3946e-02,  2.9732e-02,\n",
       "         4.3027e-01, -1.0314e-01,  5.0224e-01, -4.5368e-01, -1.9866e-01,\n",
       "        -1.2079e-01, -3.5560e-01,  1.6925e-01,  2.7737e-01,  1.6428e-01,\n",
       "         1.6426e-01,  5.9284e-02, -2.3207e-01,  2.4944e-01, -1.7201e-02,\n",
       "         4.8223e-01, -4.5565e-01, -4.8210e-01, -1.5422e-03,  4.2232e-01,\n",
       "         2.0771e-01, -5.5926e-01, -3.9452e-01,  2.5221e-02,  5.9468e-01,\n",
       "        -6.1419e-02,  8.8699e-02, -1.1301e-01, -5.0307e-01, -3.5006e-01,\n",
       "        -1.2939e-01,  6.3531e-01,  5.4795e-01,  1.5342e-01, -5.1705e-01,\n",
       "        -3.9656e-01,  1.9021e-01, -4.2690e-01, -6.9475e-01, -1.0904e-01,\n",
       "        -1.3432e-01,  3.9895e-01, -3.5712e-01, -7.4314e-01, -6.0882e-03,\n",
       "         3.4299e-01,  1.1020e-02,  1.6614e-01, -6.8052e-01,  9.1224e-02,\n",
       "        -4.8889e-01, -9.4205e-02, -2.3181e-01,  7.9942e-01,  7.1796e-02,\n",
       "         1.2049e-03, -3.5705e-01,  4.1847e-01,  3.9479e-01,  5.6705e-02,\n",
       "        -5.2086e-01, -4.7318e-01,  9.7084e-02, -2.3089e-01,  4.0087e-01,\n",
       "         6.7079e-02,  1.5894e-01, -3.3744e-02, -9.0254e-01,  2.5862e-02,\n",
       "        -2.3245e-02, -2.7045e-01, -1.8201e-01, -1.7075e-01, -2.0041e-02,\n",
       "         3.0621e-01, -1.9732e-01, -5.3083e-02,  2.6855e-01, -3.0765e-01,\n",
       "         1.0247e+00,  6.3080e-01, -2.6816e-01, -4.1515e-01,  1.5655e-01,\n",
       "        -2.7321e-02, -5.8881e-01, -2.9753e-01,  2.1695e-01,  4.0253e-02,\n",
       "        -6.5132e-02, -3.0796e-01, -5.7527e-01,  9.0476e-02,  1.4332e-03,\n",
       "         1.0483e-01,  2.7713e-01, -6.4823e-01, -6.0777e-01, -3.8200e-02,\n",
       "        -1.0598e-01, -1.9767e-02, -4.5790e-01, -1.3021e-01,  2.3310e-01,\n",
       "        -1.8358e-01, -2.3339e-01,  3.9988e-01,  3.3395e-01,  1.9575e-01,\n",
       "        -4.1358e-01, -2.6044e-01,  1.3374e-01, -2.2915e-01,  2.1064e-01,\n",
       "         3.3604e-01,  1.8339e-01, -2.2462e-01, -6.7827e-01, -2.2469e-01,\n",
       "        -1.0763e-01,  5.1627e-01,  2.4277e-01,  5.8463e-02,  2.6324e-02,\n",
       "        -3.6513e-02, -2.1908e-01, -3.4438e-01, -2.7866e-01, -2.1381e-01,\n",
       "        -2.9995e-02,  2.5739e-01, -1.1039e-01,  1.6222e-01, -2.9326e-01,\n",
       "        -7.8517e-01,  1.9880e-01, -6.2515e-01, -1.0877e-01,  5.2286e-01,\n",
       "        -1.2500e-01, -3.3252e-01,  3.5203e-01, -8.2777e-02, -5.2319e-01,\n",
       "        -2.4216e-01,  3.4107e-01, -3.0163e-01,  3.8254e-01,  2.8340e-01,\n",
       "         2.8974e-01,  6.3916e-01, -9.4404e-02, -5.6139e-02, -2.2214e-01,\n",
       "        -6.9701e-01,  3.4667e-01,  1.9931e-01,  7.3781e-02, -3.6583e-01,\n",
       "         1.1541e+00, -1.6566e-01,  6.2457e-02, -5.1479e-01, -1.1736e-01,\n",
       "         2.5846e-01, -2.9850e-01, -3.9614e-02,  1.0405e-01, -3.7153e-01,\n",
       "         4.6948e-02, -6.1504e-01,  1.2238e-01,  3.1839e-01, -1.4669e-01,\n",
       "         6.4782e-02,  2.6166e-01,  5.5823e-01, -2.4261e-01, -4.0096e-01,\n",
       "        -4.8644e-01,  4.8272e-01, -6.4788e-01, -7.4505e-01,  1.5623e-01,\n",
       "        -5.0216e-02, -3.5546e-01, -7.7972e-02, -6.1818e-01, -1.8107e-01,\n",
       "        -1.4319e-01, -1.5756e-01,  6.3509e-04,  8.9595e-02,  3.0031e-01,\n",
       "        -3.4544e-01, -6.5017e-01,  6.8267e-01, -8.3233e-02, -2.1916e-01,\n",
       "        -8.7527e-02, -2.5115e-01, -4.0423e-01,  4.6256e-01, -1.4273e-01,\n",
       "        -5.6647e-01,  3.8081e-03,  2.9517e-01,  5.2964e-02, -5.2514e-01,\n",
       "         2.8698e-02, -1.4467e-01,  2.8607e-01,  2.4937e-01,  1.6837e-01,\n",
       "        -1.1592e-01, -5.9553e-01,  4.0259e-03,  5.4180e-01,  5.3525e-01,\n",
       "        -2.3703e-01,  7.3675e-02,  1.3367e-01, -7.3234e-03,  2.5092e-01,\n",
       "         3.9331e-01,  2.8266e-01, -1.8720e-01,  9.9090e-02,  1.3131e-01,\n",
       "         3.5477e-01, -3.6425e-01, -1.6167e-01, -3.1328e-01,  3.2876e-02,\n",
       "         2.3630e-02,  1.1656e-01, -4.5066e-01, -2.6568e-01,  7.4481e-03,\n",
       "        -2.6479e-01, -2.7549e-01,  4.2854e-01,  3.6868e-01, -3.1076e-01,\n",
       "        -1.2903e-01,  4.3358e-02,  1.3960e-01,  1.1615e+00,  5.8590e-01,\n",
       "         1.6142e-02,  2.3803e-01,  2.3671e-01, -4.4702e-01,  8.4874e-02,\n",
       "         1.3983e-02,  3.2909e-01, -1.4439e-01,  4.5989e-01, -1.4051e-01,\n",
       "         5.2494e-01, -2.8975e-01,  2.1058e-01,  4.5221e-01,  8.5879e-01,\n",
       "        -1.3924e-02, -2.9671e-01, -1.5500e-01, -3.7147e-01,  5.2729e-01,\n",
       "        -3.0919e-02,  2.1347e-01,  1.5010e-01,  1.7173e-01, -6.9815e-01,\n",
       "        -8.3089e-01,  1.6723e-01,  2.4523e-01,  5.5973e-02, -7.0227e-03,\n",
       "         3.8179e-02, -1.4984e-01,  4.6547e-01,  1.7497e-01, -3.4547e-01,\n",
       "        -1.5736e-01, -5.0441e-01, -4.8342e-02,  6.3937e-01, -1.3080e-01,\n",
       "        -5.4988e-02, -4.2578e-01, -7.8268e-01, -8.6281e-02, -1.9759e-01,\n",
       "        -3.2400e-01,  4.9845e-02,  5.2079e-02, -3.8535e-01, -1.8482e-01,\n",
       "         3.5760e+00, -2.9208e-01,  2.5431e-02, -2.0109e-01, -3.7131e-01,\n",
       "        -2.7145e-01, -2.8997e-01, -1.3535e-01, -7.7710e-01,  4.2495e-01,\n",
       "        -1.4801e-01, -1.2251e-01,  2.5184e-01, -2.1614e-01,  8.9826e-01,\n",
       "         4.5289e-01,  8.0169e-01,  2.2205e-01, -5.5454e-01,  3.5593e-01,\n",
       "         3.6572e-02,  3.9620e-03, -1.9518e-02,  1.4470e-02, -6.2874e-02,\n",
       "         3.3999e-01, -8.3502e-02,  4.9041e-01, -2.0027e-01, -4.9082e-01,\n",
       "        -3.0507e-01,  1.3336e-01,  1.0373e-02, -1.6670e-02,  5.4823e-01,\n",
       "        -3.8336e-01, -5.2515e-02,  1.4215e-02, -5.5932e-01,  6.7224e-01,\n",
       "         4.3745e-02, -2.7774e-01, -1.0218e-01, -1.2220e-01,  1.3998e-01,\n",
       "         4.2122e-01, -1.0693e-01,  3.6017e-01,  7.6640e-01, -2.4507e-01,\n",
       "        -3.4753e-03, -1.6569e-01,  1.7171e-01, -9.8330e-02,  5.1497e-01,\n",
       "        -1.7249e-01, -8.1653e-02, -2.1067e-01, -6.0069e-01,  4.2976e-01,\n",
       "         2.6141e-01, -4.3807e-01,  2.8987e-01, -2.2400e-01,  3.1192e-01,\n",
       "        -5.9624e-01, -4.7788e-01, -2.3100e-01, -4.0378e-01, -1.2764e-01,\n",
       "        -3.6444e-01,  4.8758e-01,  1.7203e-01,  4.8114e-03, -5.3961e-02,\n",
       "         3.3000e-01, -5.3156e-02,  3.0372e-01, -7.3823e-01, -1.4912e-01,\n",
       "        -5.9548e-02, -3.4955e-01,  2.1387e-01,  2.4757e-01, -6.5608e-02,\n",
       "        -3.4826e-01, -1.9731e-01, -2.9918e-01, -6.3769e-01,  1.4273e-01,\n",
       "         3.1640e-01, -4.7619e-01,  4.0087e-01,  3.4488e-01,  7.0708e-03,\n",
       "        -1.6469e-01,  1.7765e-01, -2.6155e-01,  4.0472e-01,  2.8894e-01,\n",
       "         8.6257e-02,  2.3219e-01, -6.1634e-01, -3.6169e-01, -3.6060e-02,\n",
       "         2.4009e-02,  1.2714e-01,  3.0677e-01, -4.3665e-01,  8.6935e-03,\n",
       "         1.6546e-01, -2.1357e-01,  2.1379e-01, -1.8525e-01,  2.4483e-01,\n",
       "         5.3103e-01,  1.0372e-01, -4.7361e-01, -4.0787e-01, -6.0351e-01,\n",
       "         4.8974e-01,  4.9665e-01,  6.2761e-01,  3.8544e-01, -4.5834e-01,\n",
       "        -2.4816e-01, -2.5006e-02, -2.2822e-01, -1.4452e-01, -3.3509e-01,\n",
       "         2.4092e-01, -4.4614e-01,  1.1993e-01,  2.5047e-01, -5.9164e-01,\n",
       "         1.2422e-01,  1.2199e-01, -3.6180e-01,  2.0720e-01,  3.4944e-01,\n",
       "        -1.0309e+00, -3.9392e-01, -2.6832e-02, -6.3429e-02,  5.3455e-01,\n",
       "        -3.5563e-01,  6.0371e-02, -5.6523e-01,  6.0787e-01,  1.6350e-01,\n",
       "        -2.8782e-01, -1.1202e-01, -3.3369e-01,  1.8535e-01,  2.7042e-01,\n",
       "         3.8606e-01,  1.1597e-01, -5.1676e-01,  2.0943e-02, -1.5356e-01,\n",
       "        -3.3658e-01,  1.7368e-01,  2.3273e-02, -2.6864e-01, -4.4846e-01,\n",
       "         7.8617e-01,  5.2997e-01, -1.4651e-01, -3.9813e-01, -4.6576e-01,\n",
       "         9.2608e-02, -3.7534e-02,  1.9830e-01, -3.1193e-01,  4.1395e-01,\n",
       "         2.9778e-01, -3.6013e-01, -5.5635e-01, -7.0084e-05, -1.4680e-01,\n",
       "         7.2513e-02, -2.2296e-01,  1.1608e-01, -4.9160e-01,  1.8793e-01,\n",
       "         9.0846e-02, -1.1509e+00,  3.5574e-01,  2.3556e-02,  7.6901e-01,\n",
       "        -5.9200e-02, -3.5451e-01,  2.0216e-01, -4.4512e-01,  4.4571e-01,\n",
       "         1.4588e-01, -1.7456e-01,  6.1196e-02,  6.5468e-01, -5.9404e-01,\n",
       "        -1.0816e-01, -5.7862e-04, -1.5920e-01, -1.6573e-01,  1.2886e-01,\n",
       "         4.6831e-02, -1.0545e-01,  1.8185e-02,  1.3166e-01, -3.6453e-01,\n",
       "         2.5378e-01, -1.1658e-01,  5.7785e-01, -1.3820e-01,  2.6997e-01,\n",
       "        -5.7971e-02, -6.1713e-02, -4.6951e-01, -1.2242e-01,  1.2404e-01,\n",
       "         1.3212e-01,  4.5039e-01,  6.0269e-01,  5.1717e-01,  4.4143e-02,\n",
       "         1.9293e-01,  1.9430e-01,  1.8075e-01,  8.8155e-02,  1.2114e-01,\n",
       "         2.2062e-02, -2.8593e-01,  1.1942e-01,  2.7604e-01,  1.5223e-01,\n",
       "         3.3114e-01,  1.7722e-01, -3.7185e-01,  4.8174e-02,  9.0079e-01,\n",
       "         1.4798e-01, -3.1778e-01, -3.9603e-01,  1.8171e-01, -2.8475e-01,\n",
       "        -1.2374e-01,  3.0905e-01, -2.1352e-01, -2.3924e-01, -4.0690e-02,\n",
       "         1.2103e-01, -3.2472e-01, -4.9157e-01,  2.9252e-01, -4.6819e-02,\n",
       "        -1.6008e-01,  2.6496e-01, -1.1997e-01, -1.1373e+00,  1.3391e-01,\n",
       "         2.1432e-01,  1.8093e-03,  4.2436e-02, -3.9618e-01,  1.6396e-01,\n",
       "         7.8640e-02,  1.1221e-01, -1.7164e-01,  2.2467e-01,  7.0417e-02,\n",
       "         6.9455e-02,  2.2440e-01,  4.2388e-01,  7.1872e-02, -4.3146e-02,\n",
       "        -2.2068e-01, -1.7446e-01,  1.1907e-01, -3.6766e-01, -7.9476e-01,\n",
       "         3.8980e-01,  1.1653e-01, -4.8714e-01, -1.5027e-01,  2.5041e-01,\n",
       "        -1.1362e-01,  3.5694e-01, -1.5668e-01, -9.0532e-01, -3.2135e-01,\n",
       "        -5.3498e-01, -5.8673e-01, -7.7130e-01,  2.2521e-02, -6.8738e-01,\n",
       "         1.9320e-01, -6.0592e-01, -4.0695e-01, -1.5892e-01, -3.0176e-01,\n",
       "        -4.1169e-02, -2.1725e-01, -4.8850e-01,  7.0763e-02, -3.7342e-01,\n",
       "         1.4045e-01, -1.2486e-01,  1.7698e-01, -2.1787e-01,  1.1889e-01,\n",
       "        -4.2988e-01,  3.6375e-01,  3.4248e-01,  1.9964e-01,  4.1032e-01,\n",
       "         5.7028e-01, -9.7740e-01, -1.3114e-01,  4.6259e-01,  3.0366e-02,\n",
       "        -5.8338e-01, -1.8112e-01, -4.5377e-01,  9.5250e-02, -2.2267e-01,\n",
       "        -1.0053e-02,  2.7433e-02, -1.2372e-02, -6.2139e-01,  3.9817e-01,\n",
       "        -1.0093e-01, -2.4670e-01, -2.8271e-01, -4.2532e-02, -4.1951e-02,\n",
       "         5.6279e-03,  7.1032e-02,  1.8958e-01,  2.6609e-01, -4.5157e-01,\n",
       "         7.9362e-01,  7.3293e-02, -2.5611e-01, -4.6895e-01, -5.0950e-03,\n",
       "         3.0796e-01,  1.6509e-01, -2.4161e-01, -4.3716e-01, -3.1764e-02,\n",
       "        -6.2748e-03, -2.0355e-01, -7.8869e-01, -2.3338e-01,  1.9206e-01,\n",
       "         7.1935e-01, -2.8420e-01, -4.6405e-01, -4.2690e-01,  4.5800e-02,\n",
       "         4.2768e-02, -4.3917e-02,  4.2301e-02, -3.5906e-01,  1.2489e-02,\n",
       "         1.0714e-01,  3.6710e-01, -1.8725e-02, -1.8276e-01,  1.3384e-01,\n",
       "        -2.1942e-01, -9.5740e-02,  2.2831e-01,  8.1983e-01, -9.5990e-03,\n",
       "        -9.7619e-03,  2.4644e-01, -1.1537e-01, -1.0801e+00,  3.2492e-01,\n",
       "        -2.5423e-01,  5.0039e-01, -7.1320e-01, -1.9888e-01,  1.8917e-01,\n",
       "         3.5481e-01, -3.1442e-01, -3.1137e-01, -4.2393e-01,  2.9399e-01,\n",
       "         2.9627e-02,  5.6414e-02,  5.5320e-01,  8.7044e-02, -1.7917e-01,\n",
       "        -2.7355e-01, -4.5389e-01,  4.0703e-01,  1.0598e+00,  2.8235e-01,\n",
       "         3.5568e-01, -3.7110e-01, -3.9093e-01, -4.0214e-01, -2.3658e-01,\n",
       "         1.3992e-01, -6.1482e-02,  3.0152e-02,  2.5982e-01, -1.2603e-01,\n",
       "         2.9745e-01,  6.6517e-02,  2.4735e-01, -2.8303e-01, -1.6438e-01,\n",
       "        -4.1557e-01, -4.5033e-01, -3.1366e-01,  3.7734e-01,  7.8373e-02,\n",
       "         2.3995e-01,  1.7018e-01, -2.7640e-02])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pool.encode(\"hi\", convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-citizen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electra",
   "language": "python",
   "name": "electra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
